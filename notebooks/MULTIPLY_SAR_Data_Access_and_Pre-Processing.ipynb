{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIPLY SAR Data Access and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this Jupyter Notebook is to show how the MULTIPLY platform can be used to retrieve S1 SLC Data from the Data Access Component and how it can be processed into S1 GRD Data using the SAR Pre-Processing functionality.\n",
    "\n",
    "First, let's define working directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support import get_working_dir\n",
    "name = 'm1'\n",
    "\n",
    "# create and/or clear working directory\n",
    "working_dir = get_working_dir(name)\n",
    "print('Working directory is {}'.format(working_dir))\n",
    "s1_slc_directory = '{}/s1_slc'.format(working_dir)\n",
    "s1_grd_directory = '{}/s1_grd'.format(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to ensure that all data stores are set up (step over this part if you have already done it). Please also set your Earth Data Authentication if you execute this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from vm_support import set_earth_data_authentication, set_up_data_stores\n",
    "#set_up_data_stores()\n",
    "#username = ''\n",
    "#password = ''\n",
    "#set_earth_data_authentication(username, password) # to download modis data, needs only be done once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define start and end times and a region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_as_string = '2017-06-01'\n",
    "stop_time_as_string = '2017-06-10'\n",
    "roi = 'POLYGON((9.99 53.51,10.01 53.51,10.01 53.49, 9.99 53.49, 9.99 53.51))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the SAR Pre-Processing we require a config file. Let's create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vm_support import create_sar_config_file\n",
    "create_sar_config_file(temp_dir=working_dir, roi=roi, start_time=start_time_as_string, end_time=stop_time_as_string,\n",
    "                       s1_slc_directory=s1_slc_directory, s1_grd_directory=s1_grd_directory)\n",
    "config_file = f'{working_dir}/sar_config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next set up the Data Access Component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiply_data_access import DataAccessComponent\n",
    "dac = DataAccessComponent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the SAR Pre-Processing we need to have 15 products at least: 7 before and 7 after the product in question. However, the SAR Pre-Processing does not count all products as full products: If the products are located close to a border, they are counted as half products. As the determination whether a product is counted as a full or half a product is made by the SAR Pre-Processor, we need it to determine he products that are required. To do so, it is necessary to access the products, so we might already need to download.\n",
    "\n",
    "Let's start with determining the actual start date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "from multiply_orchestration import create_sym_links\n",
    "from sar_pre_processing import SARPreProcessor\n",
    "one_day = datetime.timedelta(days=1)\n",
    "\n",
    "before_sar_dir = f'{s1_slc_directory}/before'\n",
    "if not os.path.exists(before_sar_dir):\n",
    "    os.makedirs(before_sar_dir)\n",
    "\n",
    "start = datetime.datetime.strptime(start_time_as_string, '%Y-%m-%d')\n",
    "before = start\n",
    "num_before = 0\n",
    "while num_before < 7:\n",
    "    before -= one_day\n",
    "    before_date = datetime.datetime.strftime(before, '%Y-%m-%d')\n",
    "    data_urls_before = dac.get_data_urls(roi, before_date, start_time_as_string, 'S1_SLC')\n",
    "    create_sym_links(data_urls_before, before_sar_dir)\n",
    "    processor = SARPreProcessor(config=config_file, input=before_sar_dir, output=before_sar_dir)\n",
    "    list = processor.create_processing_file_list()\n",
    "    num_before = len(list[0]) + (len(list[1]) / 2.)\n",
    "logging.info(f'Set start date for collecting S1 SLC products to {before_date}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the actual end date. Take care not to set it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_sar_dir = f'{s1_slc_directory}/after'\n",
    "if not os.path.exists(after_sar_dir):\n",
    "    os.makedirs(after_sar_dir)\n",
    "\n",
    "end = datetime.datetime.strptime(stop_time_as_string, '%Y-%m-%d')\n",
    "after = end\n",
    "num_after = 0\n",
    "while num_after < 7 and after < datetime.datetime.today():\n",
    "    after += one_day\n",
    "    after_date = datetime.datetime.strftime(after, '%Y-%m-%d')\n",
    "    data_urls_after = dac.get_data_urls(roi, stop_time_as_string, after_date, 'S1_SLC')\n",
    "    create_sym_links(data_urls_after, after_sar_dir)\n",
    "    processor = SARPreProcessor(config=config_file, input=after_sar_dir, output=after_sar_dir)\n",
    "    list = processor.create_processing_file_list()\n",
    "    num_after = len(list[0]) + (len(list[1]) / 2.)\n",
    "logging.info(f'Set end date for collecting S1 SLC products to {after_date}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created extra directories for collecting the products. Let's clean up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(before_sar_dir)\n",
    "shutil.rmtree(after_sar_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are finally set to collect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_data_urls = dac.get_data_urls(roi, before_date, after_date, 'S1_SLC')\n",
    "create_sym_links(sar_data_urls, s1_slc_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been collected, we can run the actual SAR Pre-Processing. The Processing consists of three steps. The first two steps create one output product for one input product, while the third step merges information from multiple products. We can run steps 1 and 2 safely now on all the input folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SARPreProcessor(config=config_file, input=s1_slc_directory, output=s1_grd_directory)\n",
    "processor.create_processing_file_list()\n",
    "logging.info('Start Pre-processing step 1')\n",
    "processor.pre_process_step1()\n",
    "logging.info('Finished Pre-processing step 1')\n",
    "logging.info('Start Pre-processing step 2')\n",
    "processor.pre_process_step2()\n",
    "logging.info('Finished Pre-processing step 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 needs to be performed for each product separately. To do this, we need to make sure we hand in the correct products only. The output of the second step is located in an intermediate folder. First, we collect all these files and sort them temporally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "output_step2_dir = f'{s1_grd_directory}/step2'\n",
    "sorted_input_files = glob.glob(f'{output_step2_dir}/*.dim')\n",
    "sorted_input_files.sort(key=lambda x: x[len(output_step2_dir) + 18:len(output_step2_dir) + 33])\n",
    "sorted_input_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the thrird step of the SAR Pre-Processing for every product for which there are at least 7 products before and 7 products after it available. For this, it is necessary to first create the file list, then to remove all files from it that shall not be considered during this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_step3_dir = f'{s1_grd_directory}/step3'\n",
    "\n",
    "for end in range(14, len(sorted_input_files)):\n",
    "    file_list = processor.create_processing_file_list()\n",
    "    start = end-14\n",
    "    sub_list = sorted_input_files[start:end]\n",
    "    for i, list in enumerate(file_list):\n",
    "        for file in list:\n",
    "            processed_name = file.replace('.zip', '_GC_RC_No_Su_Co.dim')\n",
    "            processed_name = processed_name.replace(s1_slc_directory, output_step2_dir)\n",
    "            if processed_name not in sub_list:\n",
    "                list.remove(file)\n",
    "    processor.set_file_list(file_list)\n",
    "    logging.info(f'Start Pre-processing step 3, run {start}')\n",
    "    processor.pre_process_step3()\n",
    "    logging.info(f'Finished Pre-processing step 3, run {start}')\n",
    "    files = os.listdir(output_step3_dir)\n",
    "    for file in files:\n",
    "        shutil.move(os.path.join(output_step3_dir, file), s1_grd_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
